{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_image_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaho-morisaki/kadai/blob/main/tutorial_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 画像分類実践\n",
        "\n",
        "(colab) ページ上部の ランタイム → ランタイムのタイプを変更 → ハードウェアアクセラレータを「GPU」に変更 → 保存  \n",
        "深層学習では、簡単な計算を大量に行い学習を進めます。こういった計算にはGPUが適しています。\n",
        "\n",
        "[参考資料](https://aiacademy.jp/texts/show/?id=164)\n"
      ],
      "metadata": {
        "id": "R4xyqgNA5VzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### パッケージの準備"
      ],
      "metadata": {
        "id": "ublJpmZ6aAtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xjNaaO6pElys",
        "outputId": "ba271d63-4fbf-4c2e-992a-146cb2b8b95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.6-py2.py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from icrawler) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from icrawler) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from icrawler) (2.27.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from icrawler) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from icrawler) (8.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->icrawler) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->icrawler) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->icrawler) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->icrawler) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->icrawler) (3.4)\n",
            "Installing collected packages: icrawler\n",
            "Successfully installed icrawler-0.6.6\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージをインストール\n",
        "# !から始まる行はshell command (Pythonではない)\n",
        "!pip install icrawler\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### webから画像をかき集める：クローリング\n",
        "\n",
        "今回は猫・犬の画像分類器を作成します。猫と犬の画像が必要なので、webからかき集めてきます。収集した画像はランタイムのディスク領域に保存されるため、ランタイムのセッションを閉じると削除されます。\n",
        "\n",
        "*クローリングの結果は毎回変わるため、この方法では再現性は得られません"
      ],
      "metadata": {
        "id": "8Knh8-K3aL0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "import logging\n",
        "\n",
        "n_img = 200\n",
        "class1_name = \"cat\"\n",
        "class1_keyword = \"ねこ\"\n",
        "class2_name = \"dog\"\n",
        "class2_keyword = \"いぬ\"\n",
        "\n",
        "# 猫の画像を取得\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": class1_name}, log_level=logging.INFO, parser_threads=2, downloader_threads=4,)\n",
        "crawler.crawl(keyword=class1_keyword, max_num=n_img)  # 検索キーワードはひらがなの方がちゃんと動物が出る可能性が高い気がする\n",
        "\n",
        "# 犬の画像を取得\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": class2_name}, log_level=logging.INFO, parser_threads=2, downloader_threads=4,)\n",
        "crawler.crawl(keyword=class2_keyword, max_num=n_img)\n",
        "\n",
        "# ダウンロード用のzipファイル生成\n",
        "!zip -r $class1_name $class1_name\n",
        "!zip -r $class2_name $class2_name"
      ],
      "metadata": {
        "id": "x0YCoJH3HGBB",
        "outputId": "dfd99445-779e-4353-acd7-f05fc4559aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file http://www.wallpaperlink.com/images/wallpaper/2006/0607/02416x.jpg\n",
            "ERROR:downloader:Exception caught when downloading file http://pic12.nipic.com/20110118/2889686_203848437549_2.jpg, error: HTTPConnectionPool(host='pic12.nipic.com', port=80): Max retries exceeded with url: /20110118/2889686_203848437549_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf784513f0>, 'Connection to pic12.nipic.com timed out. (connect timeout=5)')), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file http://pic12.nipic.com/20110118/2889686_203848437549_2.jpg, error: HTTPConnectionPool(host='pic12.nipic.com', port=80): Max retries exceeded with url: /20110118/2889686_203848437549_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf783c9990>, 'Connection to pic12.nipic.com timed out. (connect timeout=5)')), remaining retry times: 1\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Taka_Shiba.jpg\n",
            "ERROR:downloader:Exception caught when downloading file http://pic12.nipic.com/20110118/2889686_203848437549_2.jpg, error: HTTPConnectionPool(host='pic12.nipic.com', port=80): Max retries exceeded with url: /20110118/2889686_203848437549_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf785c6740>, 'Connection to pic12.nipic.com timed out. (connect timeout=5)')), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 403, file http://a3.att.hudong.com/17/74/01300000167306121454749034271.jpg\n",
            "ERROR:downloader:Response status code 403, file https://akashi-t.com/wp-content/uploads/2016/10/maxresdefault.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a4.att.hudong.com/13/90/01300001389655134024901309265.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a0.att.hudong.com/78/02/01300001232089131600027630978.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a2.att.hudong.com/32/97/01300000167306121445970811958.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a4.att.hudong.com/73/13/01300000165486121466135195682.jpg\n",
            "ERROR:downloader:Exception caught when downloading file http://pic24.nipic.com/20121010/2846945_161038358183_2.jpg, error: HTTPConnectionPool(host='pic24.nipic.com', port=80): Max retries exceeded with url: /20121010/2846945_161038358183_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf7815d5d0>, 'Connection to pic24.nipic.com timed out. (connect timeout=5)')), remaining retry times: 2\n",
            "ERROR:downloader:Response status code 403, file http://a2.att.hudong.com/00/14/01300000165486121466149276908.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a1.att.hudong.com/55/30/01300000171118121739304318573.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a2.att.hudong.com/20/36/01300000164151121985361679286.jpg\n",
            "ERROR:downloader:Response status code 403, file http://a3.att.hudong.com/34/69/01300000432220134510694081625.jpg\n",
            "ERROR:downloader:Exception caught when downloading file http://pic24.nipic.com/20121010/2846945_161038358183_2.jpg, error: HTTPConnectionPool(host='pic24.nipic.com', port=80): Max retries exceeded with url: /20121010/2846945_161038358183_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf785c7640>, 'Connection to pic24.nipic.com timed out. (connect timeout=5)')), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file http://pic24.nipic.com/20121010/2846945_161038358183_2.jpg, error: HTTPConnectionPool(host='pic24.nipic.com', port=80): Max retries exceeded with url: /20121010/2846945_161038358183_2.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fbf78205e10>, 'Connection to pic24.nipic.com timed out. (connect timeout=5)')), remaining retry times: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: cat/ (stored 0%)\n",
            "  adding: cat/000004.jpg (deflated 3%)\n",
            "  adding: cat/000058.jpg (deflated 0%)\n",
            "  adding: cat/000030.jpg (deflated 0%)\n",
            "  adding: cat/000042.jpg (deflated 0%)\n",
            "  adding: cat/000033.jpg (deflated 5%)\n",
            "  adding: cat/000102.jpg (deflated 1%)\n",
            "  adding: cat/000094.jpg (deflated 6%)\n",
            "  adding: cat/000124.jpg (deflated 1%)\n",
            "  adding: cat/000003.jpg (deflated 0%)\n",
            "  adding: cat/000006.jpg (deflated 0%)\n",
            "  adding: cat/000020.jpg (deflated 1%)\n",
            "  adding: cat/000019.jpg (deflated 0%)\n",
            "  adding: cat/000071.jpg (deflated 0%)\n",
            "  adding: cat/000079.jpg (deflated 11%)\n",
            "  adding: cat/000069.jpg (deflated 0%)\n",
            "  adding: cat/000122.jpg (deflated 2%)\n",
            "  adding: cat/000140.jpg (deflated 0%)\n",
            "  adding: cat/000013.jpg (deflated 1%)\n",
            "  adding: cat/000135.jpg (deflated 0%)\n",
            "  adding: cat/000067.jpg (deflated 0%)\n",
            "  adding: cat/000169.jpg (deflated 1%)\n",
            "  adding: cat/000123.jpg (deflated 0%)\n",
            "  adding: cat/000153.jpg (deflated 0%)\n",
            "  adding: cat/000077.jpg (deflated 7%)\n",
            "  adding: cat/000119.jpg (deflated 0%)\n",
            "  adding: cat/000036.jpg (deflated 1%)\n",
            "  adding: cat/000084.jpg (deflated 0%)\n",
            "  adding: cat/000017.jpg (deflated 0%)\n",
            "  adding: cat/000032.jpg (deflated 5%)\n",
            "  adding: cat/000128.jpg (deflated 0%)\n",
            "  adding: cat/000150.jpg (deflated 12%)\n",
            "  adding: cat/000034.jpg (deflated 0%)\n",
            "  adding: cat/000050.jpg (deflated 0%)\n",
            "  adding: cat/000097.jpg (deflated 2%)\n",
            "  adding: cat/000151.jpg (deflated 2%)\n",
            "  adding: cat/000133.jpg (deflated 0%)\n",
            "  adding: cat/000023.jpg (deflated 1%)\n",
            "  adding: cat/000163.jpg (deflated 0%)\n",
            "  adding: cat/000088.jpg (deflated 3%)\n",
            "  adding: cat/000134.jpg (deflated 1%)\n",
            "  adding: cat/000129.jpg (deflated 0%)\n",
            "  adding: cat/000046.jpg (deflated 0%)\n",
            "  adding: cat/000080.jpg (deflated 0%)\n",
            "  adding: cat/000002.jpg (deflated 0%)\n",
            "  adding: cat/000148.jpg (deflated 2%)\n",
            "  adding: cat/000074.jpg (deflated 0%)\n",
            "  adding: cat/000130.jpg (deflated 0%)\n",
            "  adding: cat/000138.jpg (deflated 0%)\n",
            "  adding: cat/000157.jpg (deflated 0%)\n",
            "  adding: cat/000165.jpg (deflated 0%)\n",
            "  adding: cat/000092.jpg (deflated 0%)\n",
            "  adding: cat/000059.jpg (deflated 8%)\n",
            "  adding: cat/000118.jpg (deflated 0%)\n",
            "  adding: cat/000018.jpg (deflated 1%)\n",
            "  adding: cat/000016.jpg (deflated 0%)\n",
            "  adding: cat/000109.jpg (deflated 0%)\n",
            "  adding: cat/000073.jpg (deflated 1%)\n",
            "  adding: cat/000048.jpg (deflated 1%)\n",
            "  adding: cat/000145.jpg (deflated 0%)\n",
            "  adding: cat/000173.jpg (deflated 7%)\n",
            "  adding: cat/000100.jpg (deflated 1%)\n",
            "  adding: cat/000090.jpg (deflated 0%)\n",
            "  adding: cat/000063.jpg (deflated 1%)\n",
            "  adding: cat/000146.jpg (deflated 4%)\n",
            "  adding: cat/000076.jpg (deflated 0%)\n",
            "  adding: cat/000136.jpg (deflated 2%)\n",
            "  adding: cat/000040.jpg (deflated 0%)\n",
            "  adding: cat/000156.jpg (deflated 0%)\n",
            "  adding: cat/000112.jpg (deflated 10%)\n",
            "  adding: cat/000171.jpg (deflated 6%)\n",
            "  adding: cat/000025.jpg (deflated 1%)\n",
            "  adding: cat/000049.jpg (deflated 2%)\n",
            "  adding: cat/000127.jpg (deflated 1%)\n",
            "  adding: cat/000041.jpg (deflated 1%)\n",
            "  adding: cat/000054.jpg (deflated 0%)\n",
            "  adding: cat/000083.jpg (deflated 1%)\n",
            "  adding: cat/000044.jpg (deflated 0%)\n",
            "  adding: cat/000028.jpg (deflated 1%)\n",
            "  adding: cat/000091.jpg (deflated 0%)\n",
            "  adding: cat/000015.jpg (deflated 1%)\n",
            "  adding: cat/000141.jpg (deflated 0%)\n",
            "  adding: cat/000105.jpg (deflated 1%)\n",
            "  adding: cat/000168.jpg (deflated 2%)\n",
            "  adding: cat/000161.jpg (deflated 1%)\n",
            "  adding: cat/000110.jpg (deflated 0%)\n",
            "  adding: cat/000078.jpg (deflated 0%)\n",
            "  adding: cat/000152.jpg (deflated 6%)\n",
            "  adding: cat/000111.jpg (deflated 1%)\n",
            "  adding: cat/000007.jpg (deflated 1%)\n",
            "  adding: cat/000068.jpg (deflated 0%)\n",
            "  adding: cat/000021.jpg (deflated 0%)\n",
            "  adding: cat/000132.jpg (deflated 6%)\n",
            "  adding: cat/000066.jpg (deflated 1%)\n",
            "  adding: cat/000147.jpg (deflated 0%)\n",
            "  adding: cat/000160.jpg (deflated 23%)\n",
            "  adding: cat/000164.jpg (deflated 1%)\n",
            "  adding: cat/000012.jpg (deflated 2%)\n",
            "  adding: cat/000096.jpg (deflated 1%)\n",
            "  adding: cat/000001.jpg (deflated 12%)\n",
            "  adding: cat/000104.jpg (deflated 0%)\n",
            "  adding: cat/000106.jpg (deflated 0%)\n",
            "  adding: cat/000057.jpg (deflated 0%)\n",
            "  adding: cat/000107.jpg (deflated 0%)\n",
            "  adding: cat/000052.jpg (deflated 0%)\n",
            "  adding: cat/000087.jpg (deflated 1%)\n",
            "  adding: cat/000137.jpg (deflated 6%)\n",
            "  adding: cat/000170.jpg (deflated 0%)\n",
            "  adding: cat/000166.jpg (deflated 1%)\n",
            "  adding: cat/000139.jpg (deflated 1%)\n",
            "  adding: cat/000024.jpg (deflated 0%)\n",
            "  adding: cat/000075.jpg (deflated 1%)\n",
            "  adding: cat/000113.jpg (deflated 0%)\n",
            "  adding: cat/000154.jpg (deflated 8%)\n",
            "  adding: cat/000155.jpg (deflated 0%)\n",
            "  adding: cat/000085.jpg (deflated 0%)\n",
            "  adding: cat/000008.jpg (deflated 0%)\n",
            "  adding: cat/000099.jpg (deflated 3%)\n",
            "  adding: cat/000098.jpg (deflated 1%)\n",
            "  adding: cat/000149.jpg (deflated 3%)\n",
            "  adding: cat/000116.jpg (deflated 0%)\n",
            "  adding: cat/000022.jpg (deflated 0%)\n",
            "  adding: cat/000039.jpg (deflated 0%)\n",
            "  adding: cat/000120.jpg (deflated 30%)\n",
            "  adding: cat/000126.jpg (deflated 0%)\n",
            "  adding: cat/000162.jpg (deflated 0%)\n",
            "  adding: cat/000026.jpg (deflated 0%)\n",
            "  adding: cat/000081.jpg (deflated 1%)\n",
            "  adding: cat/000037.jpg (deflated 0%)\n",
            "  adding: cat/000029.jpg (deflated 1%)\n",
            "  adding: cat/000082.jpg (deflated 13%)\n",
            "  adding: cat/000038.jpg (deflated 0%)\n",
            "  adding: cat/000121.jpg (deflated 0%)\n",
            "  adding: cat/000055.jpg (deflated 0%)\n",
            "  adding: cat/000103.jpg (deflated 0%)\n",
            "  adding: cat/000045.jpg (deflated 0%)\n",
            "  adding: cat/000101.jpg (deflated 8%)\n",
            "  adding: cat/000131.jpg (deflated 1%)\n",
            "  adding: cat/000035.jpg (deflated 0%)\n",
            "  adding: cat/000159.jpg (deflated 0%)\n",
            "  adding: cat/000086.jpg (deflated 0%)\n",
            "  adding: cat/000005.jpg (deflated 1%)\n",
            "  adding: cat/000010.jpg (deflated 1%)\n",
            "  adding: cat/000070.jpg (deflated 0%)\n",
            "  adding: cat/000047.jpg (deflated 6%)\n",
            "  adding: cat/000114.jpg (deflated 0%)\n",
            "  adding: cat/000051.jpg (deflated 0%)\n",
            "  adding: cat/000009.jpg (deflated 5%)\n",
            "  adding: cat/000072.jpg (deflated 0%)\n",
            "  adding: cat/000143.jpg (deflated 0%)\n",
            "  adding: cat/000014.jpg (deflated 0%)\n",
            "  adding: cat/000056.jpg (deflated 0%)\n",
            "  adding: cat/000095.jpg (deflated 0%)\n",
            "  adding: cat/000108.jpg (deflated 0%)\n",
            "  adding: cat/000093.jpg (deflated 0%)\n",
            "  adding: cat/000142.jpg (deflated 3%)\n",
            "  adding: cat/000174.jpg (deflated 0%)\n",
            "  adding: cat/000115.jpg (deflated 0%)\n",
            "  adding: cat/000144.jpg (deflated 2%)\n",
            "  adding: cat/000011.jpg (deflated 0%)\n",
            "  adding: cat/000125.jpg (deflated 1%)\n",
            "  adding: cat/000062.jpg (deflated 1%)\n",
            "  adding: cat/000117.jpg (deflated 7%)\n",
            "  adding: cat/000061.jpg (deflated 0%)\n",
            "  adding: cat/000167.jpg (deflated 3%)\n",
            "  adding: cat/000172.jpg (deflated 1%)\n",
            "  adding: cat/000089.jpg (deflated 15%)\n",
            "  adding: cat/000043.jpg (deflated 0%)\n",
            "  adding: cat/000064.jpg (deflated 5%)\n",
            "  adding: cat/000060.jpg (deflated 0%)\n",
            "  adding: cat/000158.jpg (deflated 0%)\n",
            "  adding: cat/000027.jpg (deflated 1%)\n",
            "  adding: cat/000031.jpg (deflated 1%)\n",
            "  adding: cat/000053.jpg (deflated 2%)\n",
            "  adding: cat/000065.jpg (deflated 0%)\n",
            "  adding: dog/ (stored 0%)\n",
            "  adding: dog/000004.jpg (deflated 0%)\n",
            "  adding: dog/000058.jpg (deflated 0%)\n",
            "  adding: dog/000030.jpg (deflated 1%)\n",
            "  adding: dog/000042.jpg (deflated 4%)\n",
            "  adding: dog/000033.jpg (deflated 0%)\n",
            "  adding: dog/000102.jpg (deflated 4%)\n",
            "  adding: dog/000094.jpg (deflated 4%)\n",
            "  adding: dog/000124.jpg (deflated 25%)\n",
            "  adding: dog/000003.jpg (deflated 0%)\n",
            "  adding: dog/000006.jpg (deflated 6%)\n",
            "  adding: dog/000020.jpg (deflated 1%)\n",
            "  adding: dog/000019.jpg (deflated 0%)\n",
            "  adding: dog/000071.jpg (deflated 8%)\n",
            "  adding: dog/000079.jpg (deflated 0%)\n",
            "  adding: dog/000069.jpg (deflated 3%)\n",
            "  adding: dog/000122.jpg (deflated 0%)\n",
            "  adding: dog/000140.jpg (deflated 0%)\n",
            "  adding: dog/000013.jpg (deflated 0%)\n",
            "  adding: dog/000135.jpg (deflated 0%)\n",
            "  adding: dog/000067.jpg (deflated 6%)\n",
            "  adding: dog/000123.jpg (deflated 2%)\n",
            "  adding: dog/000153.jpg (deflated 21%)\n",
            "  adding: dog/000077.jpg (deflated 2%)\n",
            "  adding: dog/000119.jpg (deflated 0%)\n",
            "  adding: dog/000036.jpg (deflated 1%)\n",
            "  adding: dog/000084.jpg (deflated 2%)\n",
            "  adding: dog/000017.jpg (deflated 0%)\n",
            "  adding: dog/000032.jpg (deflated 0%)\n",
            "  adding: dog/000128.jpg (deflated 0%)\n",
            "  adding: dog/000150.jpg (deflated 0%)\n",
            "  adding: dog/000034.jpg (deflated 1%)\n",
            "  adding: dog/000050.jpg (deflated 0%)\n",
            "  adding: dog/000097.jpg (deflated 10%)\n",
            "  adding: dog/000151.jpg (deflated 3%)\n",
            "  adding: dog/000133.jpg (deflated 5%)\n",
            "  adding: dog/000023.jpg (deflated 0%)\n",
            "  adding: dog/000088.jpg (deflated 0%)\n",
            "  adding: dog/000134.jpg (deflated 1%)\n",
            "  adding: dog/000129.jpg (deflated 2%)\n",
            "  adding: dog/000046.jpg (deflated 0%)\n",
            "  adding: dog/000080.jpg (deflated 0%)\n",
            "  adding: dog/000002.jpg (deflated 10%)\n",
            "  adding: dog/000148.jpg (deflated 0%)\n",
            "  adding: dog/000074.jpg (deflated 0%)\n",
            "  adding: dog/000130.jpg (deflated 19%)\n",
            "  adding: dog/000138.jpg (deflated 0%)\n",
            "  adding: dog/000092.jpg (deflated 0%)\n",
            "  adding: dog/000059.jpg (deflated 0%)\n",
            "  adding: dog/000118.jpg (deflated 1%)\n",
            "  adding: dog/000018.jpg (deflated 3%)\n",
            "  adding: dog/000016.jpg (deflated 0%)\n",
            "  adding: dog/000109.jpg (deflated 1%)\n",
            "  adding: dog/000073.jpg (deflated 9%)\n",
            "  adding: dog/000048.jpg (deflated 16%)\n",
            "  adding: dog/000145.jpg (deflated 0%)\n",
            "  adding: dog/000100.jpg (deflated 0%)\n",
            "  adding: dog/000090.jpg (deflated 1%)\n",
            "  adding: dog/000063.jpg (deflated 5%)\n",
            "  adding: dog/000146.jpg (deflated 0%)\n",
            "  adding: dog/000076.jpg (deflated 0%)\n",
            "  adding: dog/000136.jpg (deflated 0%)\n",
            "  adding: dog/000040.jpg (deflated 41%)\n",
            "  adding: dog/000112.jpg (deflated 1%)\n",
            "  adding: dog/000025.jpg (deflated 0%)\n",
            "  adding: dog/000049.jpg (deflated 1%)\n",
            "  adding: dog/000127.jpg (deflated 11%)\n",
            "  adding: dog/000041.jpg (deflated 0%)\n",
            "  adding: dog/000054.jpg (deflated 0%)\n",
            "  adding: dog/000083.jpg (deflated 0%)\n",
            "  adding: dog/000044.jpg (deflated 1%)\n",
            "  adding: dog/000028.jpg (deflated 0%)\n",
            "  adding: dog/000091.jpg (deflated 0%)\n",
            "  adding: dog/000015.jpg (deflated 7%)\n",
            "  adding: dog/000141.jpg (deflated 0%)\n",
            "  adding: dog/000105.jpg (deflated 0%)\n",
            "  adding: dog/000110.jpg (deflated 0%)\n",
            "  adding: dog/000078.jpg (deflated 0%)\n",
            "  adding: dog/000152.jpg (deflated 16%)\n",
            "  adding: dog/000111.jpg (deflated 1%)\n",
            "  adding: dog/000007.jpg (deflated 1%)\n",
            "  adding: dog/000068.jpg (deflated 1%)\n",
            "  adding: dog/000021.jpg (deflated 0%)\n",
            "  adding: dog/000132.jpg (deflated 2%)\n",
            "  adding: dog/000066.jpg (deflated 0%)\n",
            "  adding: dog/000147.jpg (deflated 19%)\n",
            "  adding: dog/000012.jpg (deflated 1%)\n",
            "  adding: dog/000096.jpg (deflated 1%)\n",
            "  adding: dog/000001.jpg (deflated 1%)\n",
            "  adding: dog/000104.jpg (deflated 0%)\n",
            "  adding: dog/000106.jpg (deflated 2%)\n",
            "  adding: dog/000057.jpg (deflated 0%)\n",
            "  adding: dog/000107.jpg (deflated 0%)\n",
            "  adding: dog/000052.jpg (deflated 0%)\n",
            "  adding: dog/000087.jpg (deflated 1%)\n",
            "  adding: dog/000137.jpg (deflated 0%)\n",
            "  adding: dog/000139.jpg (deflated 0%)\n",
            "  adding: dog/000024.jpg (deflated 0%)\n",
            "  adding: dog/000075.jpg (deflated 0%)\n",
            "  adding: dog/000113.jpg (deflated 0%)\n",
            "  adding: dog/000154.jpg (deflated 0%)\n",
            "  adding: dog/000085.jpg (deflated 0%)\n",
            "  adding: dog/000008.jpg (deflated 0%)\n",
            "  adding: dog/000099.jpg (deflated 0%)\n",
            "  adding: dog/000098.jpg (deflated 0%)\n",
            "  adding: dog/000149.jpg (deflated 0%)\n",
            "  adding: dog/000116.jpg (deflated 0%)\n",
            "  adding: dog/000022.jpg (deflated 0%)\n",
            "  adding: dog/000039.jpg (deflated 0%)\n",
            "  adding: dog/000120.jpg (deflated 7%)\n",
            "  adding: dog/000126.jpg (deflated 5%)\n",
            "  adding: dog/000026.jpg (deflated 0%)\n",
            "  adding: dog/000081.jpg (deflated 4%)\n",
            "  adding: dog/000037.jpg (deflated 0%)\n",
            "  adding: dog/000029.jpg (deflated 0%)\n",
            "  adding: dog/000082.jpg (deflated 0%)\n",
            "  adding: dog/000038.jpg (deflated 0%)\n",
            "  adding: dog/000121.jpg (deflated 0%)\n",
            "  adding: dog/000055.jpg (deflated 0%)\n",
            "  adding: dog/000103.jpg (deflated 0%)\n",
            "  adding: dog/000045.jpg (deflated 0%)\n",
            "  adding: dog/000101.jpg (deflated 0%)\n",
            "  adding: dog/000131.jpg (deflated 6%)\n",
            "  adding: dog/000035.jpg (deflated 0%)\n",
            "  adding: dog/000086.jpg (deflated 0%)\n",
            "  adding: dog/000005.jpg (deflated 0%)\n",
            "  adding: dog/000010.jpg (deflated 0%)\n",
            "  adding: dog/000070.jpg (deflated 0%)\n",
            "  adding: dog/000047.jpg (deflated 5%)\n",
            "  adding: dog/000114.jpg (deflated 4%)\n",
            "  adding: dog/000051.jpg (deflated 3%)\n",
            "  adding: dog/000009.jpg (deflated 0%)\n",
            "  adding: dog/000072.jpg (deflated 0%)\n",
            "  adding: dog/000143.jpg (deflated 5%)\n",
            "  adding: dog/000014.jpg (deflated 1%)\n",
            "  adding: dog/000056.jpg (deflated 0%)\n",
            "  adding: dog/000095.jpg (deflated 0%)\n",
            "  adding: dog/000108.jpg (deflated 0%)\n",
            "  adding: dog/000093.jpg (deflated 3%)\n",
            "  adding: dog/000142.jpg (deflated 7%)\n",
            "  adding: dog/000115.jpg (deflated 0%)\n",
            "  adding: dog/000144.jpg (deflated 15%)\n",
            "  adding: dog/000011.jpg (deflated 0%)\n",
            "  adding: dog/000125.jpg (deflated 3%)\n",
            "  adding: dog/000062.jpg (deflated 0%)\n",
            "  adding: dog/000117.jpg (deflated 0%)\n",
            "  adding: dog/000061.jpg (deflated 0%)\n",
            "  adding: dog/000089.jpg (deflated 0%)\n",
            "  adding: dog/000043.jpg (deflated 1%)\n",
            "  adding: dog/000064.jpg (deflated 0%)\n",
            "  adding: dog/000060.jpg (deflated 1%)\n",
            "  adding: dog/000027.jpg (deflated 0%)\n",
            "  adding: dog/000031.jpg (deflated 1%)\n",
            "  adding: dog/000053.jpg (deflated 3%)\n",
            "  adding: dog/000065.jpg (deflated 3%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 収集した画像の読み込み\n",
        "\n",
        "画像をopencv (`cv2`) というモジュールで読み込みます。画像は`numpy`の`np.ndarray`という配列オブジェクトとして読み込まれます。  \n",
        "`ndarray`で気にかけておくべきことは次の3つです。\n",
        "- 配列の次元 = ランク (`ndarray.ndim`)\n",
        "- 配列の形 (`ndarray.shape`)\n",
        "- 配列のデータ型 (`ndarray.dtype`)  \n",
        "\n",
        "一方で中身の個々の値に関しては、大きい配列ですと確認が難しいため、最大値や平均値など配列全体としての情報から期待している数値が入っているか推測することになります。\n",
        "\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >\n",
        "\n",
        "低次元の配列には名前がついています。\n",
        "- 0次元: スカラー\n",
        "- 1次元: ベクトル\n",
        "- 2次元: マトリックス\n",
        "\n",
        "これらを一般化した概念をテンソルと呼びます。"
      ],
      "metadata": {
        "id": "wxEDbEYZa_mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ダウンロードした画像を全て同じ大きさにリサイズし、メモリに格納\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path  # ディスク上のファイル一覧を取得するなど、パスの操作に用いるモジュール\n",
        "\n",
        "img_size = 64\n",
        "imgs_cat = list()\n",
        "imgs_dog = list()\n",
        "\n",
        "for img_list, img_dir in ((imgs_cat, class1_name), (imgs_dog, class2_name)):\n",
        "  # 画像パス一つずつを処理\n",
        "  for impath in Path(img_dir).iterdir():\n",
        "    # 画像読み込み: cv2.imread()\n",
        "    img = cv2.imread(str(impath))\n",
        "    if img is None: continue\n",
        "    # 画像リサイズ\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    img_list.append(img)\n",
        "\n",
        "# listをndarrayに変換\n",
        "imgs_cat = np.array(imgs_cat)\n",
        "imgs_dog = np.array(imgs_dog)\n",
        "\n",
        "# テンソルの情報を確認\n",
        "print('imgs_cat.ndim =', imgs_cat.ndim)\n",
        "print('imgs_cat.shape =', imgs_cat.shape)\n",
        "print('imgs_cat.dtype =', imgs_cat.dtype)\n",
        "print('')\n",
        "print('imgs_dog.ndim =', imgs_dog.ndim)\n",
        "print('imgs_dog.shape =', imgs_dog.shape)\n",
        "print('imgs_dog.dtype =', imgs_dog.dtype)"
      ],
      "metadata": {
        "id": "4jzQGLWpHMJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`cv2.imread()`関数で読み込んだ画像のデータ型は、デフォルトでは`np.uint8`という、符号なし（=マイナスを表現できない）8 bitの整数です。[0, 255]の範囲のみ表現可能です。  \n",
        "`imgs_cat`は4次元配列で形が(n, 64, 64, 3)になっています。これは、最初の軸 (`axis = 0`) には画像の枚数nが、続く3つの次元には64 x 64 pixelの3チャネル (RGB) 画像が格納されているという意味になっています。  \n",
        "(`cv2.imread()`で読み込んだ画像は、厳密にはチャネルの並びがBGRの順番になっています。)"
      ],
      "metadata": {
        "id": "FBylpLMdgqks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 収集した画像の確認\n",
        "\n",
        "読み込んだ画像をタイル状に並べて確認します"
      ],
      "metadata": {
        "id": "XWMFHOzdfu4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像の一覧可視化関数\n",
        "import math\n",
        "import PIL\n",
        "\n",
        "def tile_show(imgs, n_col=10):\n",
        "  h, w = imgs[0].shape[:2]\n",
        "  assert all([img.shape[:2] == (h, w) for img in imgs])\n",
        "  n_row = math.ceil(len(imgs) / n_col)\n",
        "  tile = np.zeros((h*n_row, w*n_col, 3), imgs[0].dtype)\n",
        "  for i, img in enumerate(imgs):\n",
        "    row = i // n_col\n",
        "    col = i % n_col\n",
        "    y1 = row * h\n",
        "    y2 = (row+1) * h\n",
        "    x1 = col * w\n",
        "    x2 = (col+1) * w\n",
        "    tile[y1:y2, x1:x2] = img\n",
        "  \n",
        "  if not tile.dtype == np.uint8:\n",
        "    tile -= tile.min()\n",
        "    tile /= tile.max()\n",
        "    tile *= 255\n",
        "    tile = np.clip(tile.astype(np.uint8), 0, 255)\n",
        "  return PIL.Image.fromarray(tile[..., ::-1])"
      ],
      "metadata": {
        "id": "xURaDAa1Ss0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(imgs_cat))\n",
        "tile_show(imgs_cat)"
      ],
      "metadata": {
        "id": "vDsM-SrGLF84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(imgs_dog))\n",
        "tile_show(imgs_dog)"
      ],
      "metadata": {
        "id": "bAxsHhNRLGyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データ・テストデータの作成\n",
        "\n",
        "収集した画像を学習用とテスト用（精度評価用）に分け、ラベル（正解）を付与する（猫=0、犬=1）。  \n",
        "今回は **二値分類** なので、変数1つに猫か犬かという意味を持たせることができる（変数が0に近ければ猫である確率が高く、1に近ければ犬である確率が高い）。\n",
        "\n",
        "\n",
        "3クラス以上の分類 **多クラス分類** では、カテゴリの変数をone-hot encodingという手法でベクトルに変換してから用いることが一般的である。なお、二値分類でもone-hot表現は可能で、こちらの方がいい精度が出る（気がする）ので今回はこちらを使う。\n",
        "\n",
        "<img src=\"https://smart-hint.com/wp-content/uploads/2021/11/image-47.png\" width=\"500\"><a href=\"https://smart-hint.com/python/dummies/\">src</a>\n",
        "\n",
        "\n",
        "また、画像処理では、入力画像を少数で[-1, 1]の範囲に正規化して用いることが多い。すなわち、`np.uint8`[0, 255]を`np.float32`[-1, 1]に変換する。\n",
        "\n",
        "入力の画像とラベルは、次に作成するモデルの入出力とマッチするように作成する。"
      ],
      "metadata": {
        "id": "e3xlpcLasYVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 学習データとテストデータの作成\n",
        "test_rate = 0.2\n",
        "n_cat_test = int(len(imgs_cat) * test_rate)\n",
        "n_dog_test = int(len(imgs_dog) * test_rate)\n",
        "\n",
        "cat_test, cat_train = imgs_cat[:n_cat_test], imgs_cat[n_cat_test:]\n",
        "dog_test, dog_train = imgs_dog[:n_dog_test], imgs_dog[n_dog_test:]\n",
        "\n",
        "# ndarrayの結合, dtypeをfloat32に変更, [-1,1]に正規化\n",
        "train_X = np.concatenate([cat_train, dog_train], axis=0)\n",
        "train_X = train_X.astype(np.float32)\n",
        "train_X = ((train_X / 255.) - 0.5) * 2\n",
        "test_X = np.concatenate([cat_test, dog_test], axis=0)\n",
        "test_X = test_X.astype(np.float32)\n",
        "test_X = ((test_X / 255.) - 0.5) * 2\n",
        "\n",
        "# 正規化の確認。train_Xに含まれる全画素の輝度のヒストグラム\n",
        "plt.hist(train_X.flatten())\n",
        "\n",
        "# ラベルを作成。猫=0、犬=1として設定\n",
        "train_y = np.array([0.] * len(cat_train) + [1.] * len(dog_train)).astype(np.float32)\n",
        "test_y = np.array([0.] * len(cat_test) + [1.] * len(dog_test)).astype(np.float32)\n",
        "print(train_y.shape)\n",
        "\n",
        "# one-hot encoding\n",
        "train_y = tf.keras.utils.to_categorical(train_y, 2)\n",
        "test_y = tf.keras.utils.to_categorical(test_y, 2)\n",
        "print(train_y.shape)"
      ],
      "metadata": {
        "id": "VNthTJOfRiAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの作成\n",
        "\n",
        "今回はVGG16という畳み込みニューラルネットワークを用います。  \n",
        "有名なモデルはtensorflwoフレームワークに組み込まれており、呼び出すだけでモデルが使用できます。"
      ],
      "metadata": {
        "id": "xMMp8_eYtV2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "model = VGG16(\n",
        "    include_top=True, \n",
        "    weights=None, \n",
        "    input_shape=(img_size, img_size, 3), \n",
        "    classes=2,\n",
        "    )\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "u8EbqFbWtq_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  `tf.keras.Model.summary()`メソッドでは、モデルの各レイヤーのテンソルの形が表示されます (`None`は数値が任意であることを示す) 。モデルを1つの大きな関数として考えた時、重要なのは入力と出力です。  \n",
        "- input:  `input_5 (InputLayer)        [(None, 64, 64, 3)]`  \n",
        "- output:   `predictions (Dense)         (None, 2)`\n",
        "\n",
        "このモデルは、「4次元テンソルを入力されると2次元テンソルを出力する関数」と言えます。\n"
      ],
      "metadata": {
        "id": "n3bHSdeIvcj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X.shape)\n",
        "print(train_y.shape)"
      ],
      "metadata": {
        "id": "XeVBwMbjT8NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データ画像`train_X`とinputのshape、正解ラベル`train_y`のとoutputのshapeが一致していることが確認できます。"
      ],
      "metadata": {
        "id": "I9Aujv7DVoKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ニューラルネットワーク\n",
        "\n",
        "<img src=\"https://www.esector.co.jp/bpa/img/A19-034.png\" width=\"500\">\n",
        "<a href=\"https://www.esector.co.jp/bpa/A19-034.html\">src</a>\n",
        "\n",
        "ニューロンに対応する部分をノードなどと呼ぶ。たいてい1ノードだけで扱うことはなく、層のような構造に例えられるためレイヤーとも呼ばれる\n",
        "\n",
        "<img src=\"https://www.imagazine.co.jp/wp-content/uploads/2018/07/086-090_16ISno13_kiso_deep_zu003.jpg\" width=\"500\">\n",
        "<a href=\"https://www.imagazine.co.jp/%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E3%80%8C%E5%9F%BA%E7%A4%8E%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%80%8D%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99/\">src</a>\n",
        "\n",
        "畳み込みニューラルネットワーク（CNN）は、神経細胞の発火のルールを、画像の部分領域をベースにしたという点で、人間の視覚処理により近い構造を模していると考えることができる。"
      ],
      "metadata": {
        "id": "XBhfb-gp5SgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 畳み込み処理\n",
        "\n",
        "<img src=\"https://axa.biopapyrus.jp/media/CNN_Conv3.png\" width=\"500\"><a href=\"https://axa.biopapyrus.jp/deep-learning/cnn/convolution.html\">src</a>\n",
        "\n",
        "中央の3x3の配列をfilter, kernel, windowなどと呼ぶ。畳み込みは特定のパターン（模様）を抽出する操作だと考えることもでき、フィルターの内容がパターンを決定する。CNNでは、フィルターの中身を学習によって更新し、学習がうまく進むと画像内の特徴量（猫らしさ、犬らしさの指標になる模様など）が抽出できるようになる。\n",
        "\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/wrxLE.png\" width=\"500\"><a href=\"http://web.pdx.edu/~jduh/courses/Archive/geog481w07/Students/Ludwig_ImageConvolution.pdf\">src</a>  \n",
        "カーネルの違いによる畳み込み効果の例"
      ],
      "metadata": {
        "id": "WRz8y7poD9Zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの学習"
      ],
      "metadata": {
        "id": "tTt5DtMu5Whi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(train_X, train_y, epochs=30, batch_size=32)"
      ],
      "metadata": {
        "id": "avYEp9lzJp1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習過程の可視化\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(history.history['loss'], label='(left) loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(history.history['accuracy'], label='(right) accuracy', c='r')\n",
        "ax2.set_ylabel('accuracy')\n",
        "h1, l1 = ax1.get_legend_handles_labels()\n",
        "h2, l2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(h1+h2, l1+l2, loc='center left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wYo7ObPLMqlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習したモデルのテスト\n",
        "\n",
        "テストデータから1枚、画像を取り出して学習したモデルに適用してみる。  \n",
        "outputは`tf.Tensor`という型であり、`tf.Tensor.numpy()`メソッドによりnumpyの数値として情報を取り出すことができる。"
      ],
      "metadata": {
        "id": "07AzcowLZvZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [-1,1]に正規化されている画像を表示する関数\n",
        "def show_standardized_img(img):\n",
        "  img -= img.min()\n",
        "  img /= img.max()\n",
        "  img *= 255\n",
        "  img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "  return PIL.Image.fromarray(img[..., ::-1])"
      ],
      "metadata": {
        "id": "BVKZsfl4hDO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "ix = random.randint(0, len(test_X)-1)\n",
        "show_standardized_img(test_X[ix])"
      ],
      "metadata": {
        "id": "aI_94JMNhmbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(test_X[ix][np.newaxis], training=False)\n",
        "print(output)\n",
        "print(output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBnxRQ-Ke0t3",
        "outputId": "2a5c32af-e514-47ec-b4e3-8b0c2f721a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[1. 0.]], shape=(1, 2), dtype=float32)\n",
            "[[1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`output`の`shape`は (1, 2) となっているが、これは実質的には長さ2のベクトルであり、「猫である確率」と「犬である確率」の順で格納されている。"
      ],
      "metadata": {
        "id": "Bhn9sizsf3zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = output.numpy()[0]\n",
        "print('猫である確率:', f'{vector[0]:.3f}')\n",
        "print('犬である確率:', f'{vector[1]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5lCpEsgh3p",
        "outputId": "de16b804-820a-4334-afef-8a3b35cd998e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "猫である確率: 1.000\n",
            "犬である確率: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "テスト画像の表示とモデルの出力を関数でひとまとめにし、何度か実行してモデルの性能の感覚をつかむ。"
      ],
      "metadata": {
        "id": "uFj5NDJ9iZV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_and_predict(model, X):\n",
        "  ix = random.randint(0, len(X)-1)\n",
        "  output = model(X[ix][np.newaxis], training=False)\n",
        "  vector = output.numpy()[0]\n",
        "  print('猫である確率:', f'{vector[0]:.3f}')\n",
        "  print('犬である確率:', f'{vector[1]:.3f}')\n",
        "  return show_standardized_img(X[ix])"
      ],
      "metadata": {
        "id": "9Wsk3cqkZu1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_and_predict(model, test_X)"
      ],
      "metadata": {
        "id": "w-u1FCoQcLmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 精度評価\n",
        "\n",
        "混合行列 (confusion matrix) を作成する。クラス分類の問題では、正解率や感度など、1つの数字で表現される指標がいくつか存在するが、指標それぞれが見える部分と見えない部分を持つ。\n",
        "\n",
        "confusion matrixは全ての指標の元になる行列であるので、これさえ載せておけば漏れなく全体を把握することができる。  \n",
        "\n",
        "今回は「猫と判定すること」を陽性 (positive) として考える。\n",
        "\n",
        "<img src=\"https://www.naukri.com/learning/articles/wp-content/uploads/sites/11/2022/03/Confusion-matrix-1.png\" width=\"500\"><a href=\"https://www.naukri.com/learning/articles/web-stories/confusion-matrix-in-machine-learning/\">src</a>"
      ],
      "metadata": {
        "id": "ghaQAaujZ0ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrixの算出\n",
        "true = np.argmax(test_y, axis=1)\n",
        "test_y_hat = model.predict(test_X)\n",
        "pred = np.argmax(test_y_hat, axis=1)\n",
        "\n",
        "confmat = tf.math.confusion_matrix(true, pred).numpy()\n",
        "confmat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzqMpcGp5EkV",
        "outputId": "c8cdc178-edb2-40dc-b99b-a4d343eaec3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[24, 11],\n",
              "       [11, 22]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "confusion matrixは包括的な情報ではあるものの、ぱっと見ではよくわからないので指標も計算する。  \n",
        "ポイント：別名があったり日本語訳がしっくりこなかったりわかりにくい。覚えるというよりは意味合いを把握することが大事\n",
        "\n",
        "<img src=\"https://cdn-ssl-devio-img.classmethod.jp/wp-content/uploads/2015/08/Untitled.png\"><a href=\"https://dev.classmethod.jp/articles/aml-evaluation-measures/\">src</a>\n",
        "\n",
        "- 正解率 Accuracy  \n",
        "$ \\frac{TP + TN}{TP + FP + FN + TN} $\n",
        "\n",
        "- 感度 Sensitivity (Recall, True Positive Rate 再現率 検出率 真陽性率)  \n",
        "$ \\frac{TP}{TP + FN} $  \n",
        "疾病検査を例にすると、その検査が疾病をとらえる「感度・検出率」となる。(検査で病気と分かった人 / 実際に病気の人)  \n",
        "\n",
        "\n",
        "- 特異度 Specificity (True Negative Rate 真陰性率)  \n",
        "$ \\frac{TN}{FP + TN} $  \n",
        "検査で言えば、検査で陰性だった人 / 実際に陰性の人\n",
        "\n",
        "- 陽性的中率 Positive Predictive Value (Precision 適合度 精度)  \n",
        "$ \\frac{TP}{TP + FP} $  \n",
        "「陽性適中率が高い」とは、「検査結果が陽性と判定された場合に、真の陽性（有病者）である確率が高い」という意味。\n",
        "\n",
        "\n",
        "他にも山ほどあります [wikipedia](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)"
      ],
      "metadata": {
        "id": "HVle16bg_lG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[TP, FP], \n",
        " [FN, TN]] = confmat\n",
        "\n",
        "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "specificity = TN / (FP + TN)\n",
        "precision = TP / (TP + FP)\n",
        "\n",
        "print('正解率', accuracy)\n",
        "print('感度', sensitivity)\n",
        "print('特異度', specificity)\n",
        "print('陽性的中率', precision)"
      ],
      "metadata": {
        "id": "t3mZ9R4E8LNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習モデルの実利用\n",
        "\n",
        "学習モデルはデータセット外の実データに適用して初めて役立ちます。  \n",
        "ここでは例として、画像URLを受け取ると猫か犬か判定してくれるwebサービスを想定します。\n",
        "\n",
        "(実際のサービス例：[Google Lens](https://lens.google/intl/ja/#:~:text=%E6%A4%8D%E7%89%A9%E3%82%84%E5%8B%95%E7%89%A9,%E7%A2%BA%E8%AA%8D%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82))"
      ],
      "metadata": {
        "id": "A9u71LGZKh8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urlから画像を読みこむcv2u.urlread()を使用するためパッケージをインストール\n",
        "!pip install python-opencv-utils\n",
        "import cv2u"
      ],
      "metadata": {
        "id": "17bYCMNDNNZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 犬猫画像判定サービス { run: \"auto\", vertical-output: true }\n",
        "\n",
        "labels = {0: 'ネコ', 1: 'イヌ'}\n",
        "\n",
        "URL = '' #@param {type:\"string\"}\n",
        "try:\n",
        "  img = cv2u.urlread(URL)\n",
        "except ValueError:\n",
        "  img = None\n",
        "\n",
        "if img is None:\n",
        "  if URL:\n",
        "    print('!!! 画像が見つかりません !!!')\n",
        "\n",
        "else:\n",
        "  X = cv2.resize(img, (img_size, img_size))\n",
        "  X = ((X.astype(np.float32) / 255.) - 0.5) * 2\n",
        "  X = X[np.newaxis]\n",
        "  y = model.predict(X)\n",
        "  label = labels[np.argmax(y[0])]\n",
        "  score = np.max(y) * 100\n",
        "  print(f'これは {score: .1f}% 「{label}」 です！')\n",
        "  plt.imshow(img[..., ::-1])\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Kfig0nYWL8VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### より良い精度を目指して\n",
        "\n",
        "- データセットの量を増やす  \n",
        "今回のデモでは各クラスの画像の最大量は200枚としました。一般的には学習量を増やせば精度が向上する傾向にあります。\n",
        "\n",
        "- 質の高いデータセットの準備  \n",
        "画像データを可視化するとわかりますが、スクレイピングで得られた画像には全く関係ないものがある程度含まれます。これらを取り除き、質の高いデータセットを構築することが精度向上につながります。\n",
        "\n",
        "- データの水増し (データ拡張、Data Augmentation)  \n",
        "データセットの量を増やすのは人力が多く骨が折れます。水増しの手法では、1枚の画像を疑似的に増幅し、枚数が増えたように見せかけます。  \n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-7565e09ecc4a93604b9c26c8c2a89bf9-pjlq\" width=\"500\"><a href=\"https://www.quora.com/What-is-data-augmentation-in-CNN\">src</a>\n",
        "\n",
        "- モデルの変更  \n",
        "VGG16は2014年に開発されたモデルで古いため、よりよいモデルが多く開発されています。   \n",
        "代表的なもの: ResNet (2015), EfficientNet (2019)"
      ],
      "metadata": {
        "id": "pZHsJ_oZZdAy"
      }
    }
  ]
}